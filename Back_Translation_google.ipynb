{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import platform\n",
        "import gc\n",
        "import sys\n",
        "import argparse\n",
        "from glob import glob\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from joblib import Parallel, delayed\n",
        "import re\n",
        "import random\n",
        "import requests\n",
        "import urllib.request\n",
        "import json\n",
        "from copy import deepcopy\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "import transformers\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedTokenizerFast\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer, SFTTrainer\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "from typing import Optional, Dict, Sequence\n",
        "from Korpora import Korpora\n",
        "from Korpora import KowikiTextKorpus, KorNLIKorpus\n",
        "# from googletrans import Translator\n",
        "from dask import bag, diagnostics\n",
        "\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "AKNLZMCMfj9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_system_specs():\n",
        "    # Check if CUDA is available\n",
        "    is_cuda_available = torch.cuda.is_available()\n",
        "    print(\"CUDA Available:\", is_cuda_available)\n",
        "# Get the number of available CUDA devices\n",
        "    num_cuda_devices = torch.cuda.device_count()\n",
        "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
        "    if is_cuda_available:\n",
        "        for i in range(num_cuda_devices):\n",
        "            # Get CUDA device properties\n",
        "            device = torch.device('cuda', i)\n",
        "            print(f\"--- CUDA Device {i} ---\")\n",
        "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
        "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
        "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
        "    # Get CPU information\n",
        "    print(\"--- CPU Information ---\")\n",
        "    print(\"Processor:\", platform.processor())\n",
        "    print(\"System:\", platform.system(), platform.release())\n",
        "    print(\"Python Version:\", platform.python_version())\n",
        "print_system_specs()"
      ],
      "metadata": {
        "id": "bQYCApdUflnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "zD_KMuPFfoYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aNQgi1Rhfpgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load to Data\n",
        "data_location = '/content/drive/MyDrive/llm'\n",
        "data_path = Path(data_location)\n",
        "\n",
        "train = pd.read_csv(data_path / 'train.csv')"
      ],
      "metadata": {
        "id": "cBq5KIQffqUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6gPZw_dfAFY"
      },
      "outputs": [],
      "source": [
        "# translate-back_translate\n",
        "\n",
        "def back_translate(sentence, lang, PROB = 1):\n",
        "  #languages = ['ko', 'en', 'fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi', 'sw', 'vi', 'es', 'el', 'ja']\n",
        "\n",
        "  translator = Translator()\n",
        "\n",
        "  org_lang = translator.detect(sentence).lang # ko\n",
        "\n",
        "  #random_language = np.random.choice([lang for lang in languages if lang is not org_lang])\n",
        "\n",
        "  if lang is not org_lang:\n",
        "\n",
        "    translated = translator.translate(sentence, dest = lang).text\n",
        "    translated_back = translator.translate(translated, dest = org_lang).text\n",
        "\n",
        "  else:\n",
        "\n",
        "    translated_back = sentence\n",
        "\n",
        "  # if np.random.uniform(0, 1) <= PROB:\n",
        "  #   output_sentence = translated_back\n",
        "  # else:\n",
        "  #   output_sentence = sentence\n",
        "\n",
        "  return translated_back\n",
        "\n",
        "# parallel apply\n",
        "\n",
        "def back_translate_parallel1(dataset, translate_column, lang, save_file = True):\n",
        "  translate_bag = bag.from_sequence(dataset[translate_column].tolist()).map(lambda x: back_translate_papago(x, lang = lang))\n",
        "\n",
        "  with diagnostics.ProgressBar():\n",
        "    bag_completed = translate_bag.compute()\n",
        "\n",
        "  dataset[f'{translate_column}_tranlsate'] = bag_completed\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def back_translate_parallel2(dataset, translate_column, lang, save_file = True):\n",
        "  try:\n",
        "    dataset[f'{translate_column}_translate'] = dataset[f'{translate_column}'].progress_apply(lambda x: back_translate_papago(x, 'en'))\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error occurred: {str(e)}\")\n",
        "    if save_file:\n",
        "      save_path = '/content/back_translate.csv'\n",
        "      if not os.path.exists(f'{save_path}'):\n",
        "        dataset.to_csv(f'{save_path}', index = False, mode = 'w', encoding = 'utf-8-sig', header = False)\n",
        "      else:\n",
        "        dataset.to_csv(f'{save_path}', index = False, mode = 'a', encoding = 'utf-8-sig', header = False)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import googletrans\n",
        "# googletrans.LANGUAGES\n",
        "\n",
        "train_aug = back_translate_parallel2(dataset = train, translate_column = '답변_1', lang = 'en')"
      ],
      "metadata": {
        "id": "R4B5jMgVfR6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}